{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "QAF3e66QyJhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9226deb-1976-4ffb-944b-98dcf0fc4fe4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SK_IH1gn4fvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "Xd5eKVLcFoiL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the data\n"
      ],
      "metadata": {
        "id": "KCIIPOEwQhYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividing the data in test and training data but first getting total number of rows, then only adding 90% of it to training data.\n",
        "data=[]\n",
        "number_of_rows=0\n",
        "with open('A_Z Handwritten Data.csv', newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      number_of_rows+=1\n",
        "      data.append(list(row))"
      ],
      "metadata": {
        "id": "0hlu1l2NFFWs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segregating the data"
      ],
      "metadata": {
        "id": "WbOUs3sXck_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# segregating the read data\n",
        "# train_data=[]\n",
        "# test_data=[]\n",
        "# counter=0\n",
        "# for i in data:\n",
        "#   counter+=1\n",
        "#   if counter<=(0.9*number_of_rows):\n",
        "#     train_data.append(i)\n",
        "#   else: \n",
        "#     test_data.append(i)\n",
        "\n",
        "#THIS IS COMMENTED OUT FOR THE REASON EXPLAINED IN BELOW COMMENTS"
      ],
      "metadata": {
        "id": "w2eY7fMqSMLh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Now the next step is to calculate p(y=k) where k belongs to [0,25]\n",
        "# dict1={}\n",
        "# counter_list=[]  \n",
        "# #will store the first column of each row and then we will use counter to count number of times a particular one appears and then divide by total number to get probability by favourable/total\n",
        "\n",
        "# for i in train_data:\n",
        "#   counter_list.append(i[0])\n",
        "\n",
        "# #Now our next step is to assign the number and the probability of each as key value pair in dictionary, we are using dictionary because it uses hash mapping ang take less time to fetch data\n",
        "# counter_set=set(counter_list)   #just to remove the duplicates and see all the labels that have been covered in the training data\n",
        "# print(counter_set)\n",
        "# #OUTPUT=['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '3', '4', '5', '6', '7', '8', '9']\n",
        "# #so we can see if we use training data then not all the labels between 0 to 25 are covered. So for calculating P(Y=k) we need to first shuffle the data and then segregate it and that's why above part was commented out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnvsfEJDRO-2",
        "outputId": "7e5aca90-0fe6-4e34-a62a-5ca42f680cdb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '3', '4', '5', '6', '7', '8', '9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "xlJjCa66be7K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#segregating the read data\n",
        "train_data=[]\n",
        "test_data=[]\n",
        "counter=0\n",
        "for i in data:\n",
        "  counter+=1\n",
        "  if counter<=(0.9*number_of_rows):\n",
        "    train_data.append(i)\n",
        "  else: \n",
        "    test_data.append(i)"
      ],
      "metadata": {
        "id": "S7y8gwYrbkcq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation P(Y=k)"
      ],
      "metadata": {
        "id": "R1_ZF7TOcopL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now the next step is to calculate p(y=k) where k belongs to [0,25]\n",
        "dict1={}\n",
        "counter_list=[]  \n",
        "#will store the first column of each row and then we will use counter to count number of times a particular one appears and then divide by total number to get probability by favourable/total\n",
        "\n",
        "for i in train_data:\n",
        "  counter_list.append(i[0])\n",
        "\n",
        "#Now our next step is to assign the number and the probability of each as key value pair in dictionary, we are using dictionary because it uses hash mapping ang take less time to fetch data\n",
        "counter_set=set(counter_list)   #just to remove the duplicates and see all the labels that have been covered in the training data. These will help in adding keys to the dictionary"
      ],
      "metadata": {
        "id": "C7pxjodkbsou"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating P(y=k) and adding to dictionary\n",
        "for i in counter_set:\n",
        "  probability=counter_list.count(i)/len(counter_list)\n",
        "  dict1[i]=round(probability,6)"
      ],
      "metadata": {
        "id": "iLtfSPcKcRtf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xLWO2UQdEIX",
        "outputId": "45de7825-3661-4172-cb68-10f0c1049287"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'15': 0.051989, '13': 0.051091, '8': 0.002983, '18': 0.129861, '4': 0.030757, '19': 0.060205, '11': 0.031044, '1': 0.023359, '16': 0.015564, '5': 0.003076, '12': 0.032974, '25': 0.01633, '9': 0.022983, '20': 0.077937, '24': 0.029137, '3': 0.027321, '23': 0.016727, '22': 0.028982, '0': 0.03732, '14': 0.155469, '2': 0.062929, '17': 0.030778, '6': 0.0154, '10': 0.015119, '21': 0.011292, '7': 0.019373}\n"
          ]
        }
      ]
    }
  ]
}